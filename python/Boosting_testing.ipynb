{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "@author: Chia-Ta Tsai\n",
    "#blended with two GBMs (XGBoost and LightGBM)\n",
    "#feature transformation was originally from the1owl's kernel, 'Natural Growth Patterns' but refactered afterward. Forked from\n",
    "#https://www.kaggle.com/the1owl/natural-growth-patterns-fractals-of-nature\n",
    "\n",
    "**Updates\n",
    "ver07: add garbage collect\n",
    "ver06: reverted to ver04 and added footnotes\n",
    "ver05: LB 0.2093\n",
    "ver04: LB 0.2021\n",
    "\"\"\"\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "#\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing\n",
    "import datetime as dt\n",
    "#\n",
    "from random import choice, sample, shuffle, uniform, seed\n",
    "from math import exp, expm1, log1p, log10, log2, sqrt, ceil, floor, isfinite, isnan\n",
    "from itertools import combinations\n",
    "#import for image processing\n",
    "import cv2\n",
    "from scipy.stats import kurtosis, skew\n",
    "from scipy.ndimage import laplace, sobel\n",
    "#evaluation\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import log_loss\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "STORE_PATH = \"../data/boost/\"\n",
    "\n",
    "\n",
    "from skimage import filters\n",
    "from skimage import restoration\n",
    "\n",
    "def normalize_img(img):\n",
    "    norm_img = np.copy(img)\n",
    "    min_val = np.min(img)\n",
    "    norm_img = norm_img - min_val\n",
    "    max_val = np.max(norm_img)\n",
    "    norm_img = (norm_img / abs(max_val))\n",
    "    return norm_img\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "def read_jason(file='', loc='../input/'):\n",
    "\n",
    "    df = pd.read_json('{}{}'.format(loc, file))\n",
    "    df['inc_angle'] = df['inc_angle'].replace('na', -1).astype(float)\n",
    "    #print(df['inc_angle'].value_counts())\n",
    "    \n",
    "    band1 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in df[\"band_1\"]])\n",
    "    band2 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in df[\"band_2\"]])\n",
    "#    band1_imf1 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in df[\"band_1_imf_1\"]])\n",
    "#    band1_imf2 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in df[\"band_1_imf_2\"]])\n",
    "#    band2_imf1 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in df[\"band_2_imf_1\"]])\n",
    "#    band2_imf2 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in df[\"band_2_imf_2\"]])\n",
    "    df = df.drop(['band_1', 'band_2'], axis=1)\n",
    "    \n",
    "#    bands = np.stack((band1, band2,  0.5 * (band1 + band2), band1_imf1, band1_imf2, band2_imf1, band2_imf2), axis=-1)\n",
    "#    del band1, band2, band1_imf1, band1_imf2, band2_imf1, band2_imf2\n",
    "\n",
    "    bands = np.stack((band1, band2, 0.5 * (band1 + band2)), axis=-1)\n",
    "    del band1, band2\n",
    "    \n",
    "    return df, bands\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "def run_lgb(params={}, lgb_train=None, lgb_valid=None, lgb_test=None, test_ids=None, nr_round=2000, min_round=100, file=''):\n",
    "\n",
    "    print('\\nLightGBM: {}'.format(params['boosting'])) \n",
    "    model2 = lgb.train(params, \n",
    "                       lgb_train, \n",
    "                       nr_round, \n",
    "                       lgb_valid, \n",
    "                       verbose_eval=50, early_stopping_rounds=min_round)\n",
    "    \n",
    "    pred = model2.predict(lgb_test, num_iteration=model2.best_iteration)\n",
    "    #\n",
    "    subm = pd.DataFrame({'id': test_ids, 'is_iceberg': pred})\n",
    "    subm.to_csv(file, index=False, float_format='%.6f')\n",
    "    #   \n",
    "    df = pd.DataFrame({'feature':model2.feature_name(), 'importances': model2.feature_importance()})\n",
    "    \n",
    "    return pred, df\n",
    "\n",
    "def get_stats(img):\n",
    "    mean = np.mean(img)\n",
    "    std = np.std(img)\n",
    "    max_v = np.max(img)\n",
    "    min_v = np.min(img)\n",
    "    median = np.median(img)\n",
    "    return [mean, std, max_v, median, min_v, \\\n",
    "            (max_v - median), (max_v - min_v), (median - min_v), \\\n",
    "            ((max_v - median) / std), ((max_v - min_v) / std), ((median - min_v) / std) \\\n",
    "            ]\n",
    "        \n",
    "###############################################################################\n",
    "#forked from\n",
    "#https://www.kaggle.com/the1owl/planet-understanding-the-amazon-from-space/natural-growth-patterns-fractals-of-nature/notebook\n",
    "def img_to_stats(paths):\n",
    "    \n",
    "    img_id, img = paths[0], paths[1]\n",
    "    \n",
    "    #ignored error    \n",
    "    np.seterr(divide='ignore', invalid='ignore')\n",
    "    \n",
    "    bins = 20\n",
    "    scl_min, scl_max = -50, 50\n",
    "    opt_poly = True\n",
    "    #opt_poly = False\n",
    "    \n",
    "    try:\n",
    "        st = []\n",
    "        st_interv = []\n",
    "        hist_interv = []\n",
    "        for i in range(img.shape[2]):\n",
    "            img_sub = np.squeeze(img[:, :, i])\n",
    "            \n",
    "            # More filters\n",
    "            #norm_img = normalize_img(img_sub)\n",
    "            #gaussian_filter = filters.gaussian(norm_img, sigma=2)\n",
    "            #med_filter = filters.median(norm_img, np.ones((3, 3)))\n",
    "            #tv_filter = restoration.denoise_tv_chambolle(norm_img, weight=0.1)\n",
    "            \n",
    "            #median, max and min\n",
    "            sub_st = []\n",
    "            #sub_st += [np.mean(img_sub), np.std(img_sub), np.max(img_sub), np.median(img_sub), np.min(img_sub)]\n",
    "            #sub_st += [(sub_st[2] - sub_st[3]), (sub_st[2] - sub_st[4]), (sub_st[3] - sub_st[4])] \n",
    "            #sub_st += [(sub_st[-3] / sub_st[1]), (sub_st[-2] / sub_st[1]), (sub_st[-1] / sub_st[1])] #normalized by stdev\n",
    "            sub_st += get_stats(img_sub)\n",
    "            #sub_st += get_stats(gaussian_filter)\n",
    "            #sub_st += get_stats(med_filter)\n",
    "            #sub_st += get_stats(tv_filter)\n",
    "            st += sub_st\n",
    "            #Laplacian, Sobel, kurtosis and skewness\n",
    "            st_trans = []\n",
    "            st_trans += [laplace(img_sub, mode='reflect', cval=0.0).ravel().var()] #blurr\n",
    "            sobel0 = sobel(img_sub, axis=0, mode='reflect', cval=0.0).ravel().var()\n",
    "            sobel1 = sobel(img_sub, axis=1, mode='reflect', cval=0.0).ravel().var()\n",
    "\n",
    "            \n",
    "            #gfv = gaussian_filter.ravel().var()\n",
    "            #mfv = med_filter.ravel().var()\n",
    "            #tvf = tv_filter.ravel().var()\n",
    "            \n",
    "\n",
    "            st_trans += [sobel0, sobel1]\n",
    "            #st_trans += [img_sub.ravel().var()]\n",
    "            #st_trans += [gfv, mfv, tvf]\n",
    "            st_trans += [kurtosis(img_sub.ravel()), skew(img_sub.ravel())]\n",
    "            \n",
    "            if opt_poly:\n",
    "                st_interv.append(sub_st)\n",
    "                #\n",
    "                st += [x * y for x, y in combinations(st_trans, 2)]\n",
    "                st += [x + y for x, y in combinations(st_trans, 2)]\n",
    "                st += [x - y for x, y in combinations(st_trans, 2)]                \n",
    " \n",
    "            #hist\n",
    "            #hist = list(cv2.calcHist([img], [i], None, [bins], [0., 1.]).flatten())\n",
    "            hist = list(np.histogram(img_sub, bins=bins, range=(scl_min, scl_max))[0])\n",
    "            hist_interv.append(hist)\n",
    "            st += hist\n",
    "            st += [hist.index(max(hist))] #only the smallest index w/ max value would be incl\n",
    "            st += [np.std(hist), np.max(hist), np.median(hist), (np.max(hist) - np.median(hist))]\n",
    "\n",
    "        if opt_poly:\n",
    "            for x, y in combinations(st_interv, 2):\n",
    "                st += [float(x[j]) * float(y[j]) for j in range(len(st_interv[0]))]\n",
    "\n",
    "            for x, y in combinations(hist_interv, 2):\n",
    "                hist_diff = [x[j] * y[j] for j in range(len(hist_interv[0]))]\n",
    "                st += [hist_diff.index(max(hist_diff))] #only the smallest index w/ max value would be incl\n",
    "                st += [np.std(hist_diff), np.max(hist_diff), np.median(hist_diff), (np.max(hist_diff) - np.median(hist_diff))]\n",
    "                \n",
    "        #correction\n",
    "        nan = -999\n",
    "        for i in range(len(st)):\n",
    "            if isnan(st[i]) == True:\n",
    "                st[i] = nan\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    return [img_id, st]\n",
    "\n",
    "\n",
    "def extract_img_stats_new(paths):\n",
    "    imf_d = {}\n",
    "    p = Pool(8) #(cpu_count())\n",
    "    ret = p.map(img_to_stats, paths)\n",
    "    for i in tqdm(range(len(ret)), miniters=100):\n",
    "        imf_d[ret[i][0]] = ret[i][1]\n",
    "    paths = [(k,v) for k,v,_,_,_,_ in paths]\n",
    "    ret = []\n",
    "    fdata = [imf_d[i] for i, j in paths]\n",
    "    return np.array(fdata, dtype=np.float32)\n",
    "\n",
    "\n",
    "def process_new(df, bands):\n",
    "            \n",
    "    band_max = df['band_1_max'].tolist() + df['band_2_max'].tolist()\n",
    "    band_mean = df['band_1_mean'].tolist() + df['band_2_mean'].tolist()\n",
    "    band_variance = df['band_1_variance'].tolist() + df['band_2_variance'].tolist()\n",
    "    band_numobj = df['band_1_numobj'].tolist() + df['band_2_numobj'].tolist()\n",
    "\n",
    "    data = extract_img_stats_new([(k, v, bmx, bmn, bvr, bno) for k, v, bmx, bmn, bvr, bno in zip(df['id'].tolist(), bands, band_max, band_mean, band_variance, band_numobj)]); gc.collect()\n",
    "    data = np.concatenate([data, df['inc_angle'].values[:, np.newaxis]], axis=-1); gc.collect()\n",
    "\n",
    "    print(data.shape)\n",
    "    return data\n",
    "\n",
    "def extract_img_stats(paths):\n",
    "    imf_d = {}\n",
    "    p = Pool(8) #(cpu_count())\n",
    "    ret = p.map(img_to_stats, paths)\n",
    "    for i in tqdm(range(len(ret)), miniters=100):\n",
    "        imf_d[ret[i][0]] = ret[i][1]\n",
    "\n",
    "    ret = []\n",
    "    fdata = [imf_d[i] for i, j in paths]\n",
    "    return np.array(fdata, dtype=np.float32)\n",
    "\n",
    "\n",
    "def process(df, bands):\n",
    "\n",
    "    data = extract_img_stats([(k, v) for k, v in zip(df['id'].tolist(), bands)]); gc.collect()\n",
    "    data = np.concatenate([data, df['inc_angle'].values[:, np.newaxis]], axis=-1); gc.collect()\n",
    "\n",
    "    print(data.shape)\n",
    "    return data\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "def save_blend(preds={}, loc='./'):\n",
    "    \n",
    "    target = 'is_iceberg'\n",
    "    \n",
    "    w_total = 0.0\n",
    "    blend = None\n",
    "    df_corr = None\n",
    "    print('\\nBlending...')\n",
    "    for k, v in preds.items():\n",
    "        if blend is None:\n",
    "            blend = pd.read_csv('{0}/{1}'.format(loc, k))\n",
    "            print('load: {0}, w={1}'.format(k, v))\n",
    "            \n",
    "            df_corr = pd.DataFrame({'id': blend['id'].tolist()})\n",
    "            df_corr[k[16:-4]] = blend[target]\n",
    "            \n",
    "            w_total += v\n",
    "            blend[target] = blend[target] * v\n",
    "                \n",
    "        else:\n",
    "            preds_tmp = pd.read_csv('{0}/{1}'.format(loc, k))\n",
    "            preds_tmp = blend[['id']].merge(preds_tmp, how='left', on='id')\n",
    "            print('load: {0}, w={1}'.format(k, v))\n",
    "            df_corr[k[16:-4]] = preds_tmp[target]\n",
    "            \n",
    "            w_total += v\n",
    "            blend[target] += preds_tmp[target] * v\n",
    "            del preds_tmp\n",
    "            \n",
    "    print('\\n{}'.format(df_corr.corr()), flush=True)\n",
    "    #write submission\n",
    "    blend[target] = blend[target] / w_total\n",
    "    print('\\nPreview: \\n{}'.format(blend.head()), flush=True)\n",
    "    blend.to_csv(STORE_PATH + '{}subm_blend{:03d}_{}.csv'.format(loc, len(preds), tmp), index=False, float_format='%.6f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1604/1604 [00:00<00:00, 1179464.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1604, 247)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 8424/8424 [00:00<00:00, 1428569.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8424, 247)\n",
      "\n",
      "round 0001 of 0003, seed=25\n",
      "splitted: (1283, 247), (321, 247)\n",
      "[0]\ttrain-error:0.141076\tvalid-error:0.205607\n",
      "Multiple eval metrics have been passed: 'valid-error' will be used for early stopping.\n",
      "\n",
      "Will train until valid-error hasn't improved in 100 rounds.\n",
      "[50]\ttrain-error:0.085737\tvalid-error:0.124611\n",
      "[100]\ttrain-error:0.051442\tvalid-error:0.102804\n",
      "[150]\ttrain-error:0.030398\tvalid-error:0.090343\n",
      "[200]\ttrain-error:0.012471\tvalid-error:0.087227\n",
      "[250]\ttrain-error:0.005456\tvalid-error:0.090343\n",
      "Stopping. Best iteration:\n",
      "[184]\ttrain-error:0.018706\tvalid-error:0.084112\n",
      "\n",
      "\n",
      "LightGBM: gbdt\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's binary_error: 0.115265\n",
      "[100]\tvalid_0's binary_error: 0.0934579\n",
      "[150]\tvalid_0's binary_error: 0.0872274\n",
      "[200]\tvalid_0's binary_error: 0.0903427\n",
      "[250]\tvalid_0's binary_error: 0.0778816\n",
      "[300]\tvalid_0's binary_error: 0.0872274\n",
      "Early stopping, best iteration is:\n",
      "[240]\tvalid_0's binary_error: 0.0778816\n",
      "\n",
      "LightGBM: dart\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's binary_error: 0.11215\n",
      "[100]\tvalid_0's binary_error: 0.105919\n",
      "[150]\tvalid_0's binary_error: 0.102804\n",
      "[200]\tvalid_0's binary_error: 0.0934579\n",
      "[250]\tvalid_0's binary_error: 0.0903427\n",
      "[300]\tvalid_0's binary_error: 0.0872274\n",
      "[350]\tvalid_0's binary_error: 0.0809969\n",
      "[400]\tvalid_0's binary_error: 0.0841121\n",
      "[450]\tvalid_0's binary_error: 0.0809969\n",
      "[500]\tvalid_0's binary_error: 0.0841121\n",
      "[550]\tvalid_0's binary_error: 0.0841121\n",
      "Early stopping, best iteration is:\n",
      "[460]\tvalid_0's binary_error: 0.0716511\n",
      "\n",
      "round 0002 of 0003, seed=25\n",
      "splitted: (1283, 247), (321, 247)\n",
      "[0]\ttrain-error:0.154326\tvalid-error:0.199377\n",
      "Multiple eval metrics have been passed: 'valid-error' will be used for early stopping.\n",
      "\n",
      "Will train until valid-error hasn't improved in 100 rounds.\n",
      "[50]\ttrain-error:0.074825\tvalid-error:0.133956\n",
      "[100]\ttrain-error:0.053001\tvalid-error:0.127726\n",
      "[150]\ttrain-error:0.025721\tvalid-error:0.11838\n",
      "[200]\ttrain-error:0.010912\tvalid-error:0.11215\n",
      "[250]\ttrain-error:0.003118\tvalid-error:0.109034\n",
      "[300]\ttrain-error:0.001559\tvalid-error:0.096573\n",
      "[350]\ttrain-error:0\tvalid-error:0.093458\n",
      "[400]\ttrain-error:0\tvalid-error:0.093458\n",
      "Stopping. Best iteration:\n",
      "[340]\ttrain-error:0\tvalid-error:0.090343\n",
      "\n",
      "\n",
      "LightGBM: gbdt\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's binary_error: 0.121495\n",
      "[100]\tvalid_0's binary_error: 0.105919\n",
      "[150]\tvalid_0's binary_error: 0.105919\n",
      "[200]\tvalid_0's binary_error: 0.102804\n",
      "[250]\tvalid_0's binary_error: 0.0996885\n",
      "[300]\tvalid_0's binary_error: 0.0965732\n",
      "Early stopping, best iteration is:\n",
      "[243]\tvalid_0's binary_error: 0.0965732\n",
      "\n",
      "LightGBM: dart\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's binary_error: 0.127726\n",
      "[100]\tvalid_0's binary_error: 0.127726\n",
      "[150]\tvalid_0's binary_error: 0.11838\n",
      "[200]\tvalid_0's binary_error: 0.109034\n",
      "[250]\tvalid_0's binary_error: 0.0965732\n",
      "[300]\tvalid_0's binary_error: 0.0934579\n",
      "[350]\tvalid_0's binary_error: 0.0996885\n",
      "[400]\tvalid_0's binary_error: 0.0996885\n",
      "Early stopping, best iteration is:\n",
      "[301]\tvalid_0's binary_error: 0.0903427\n",
      "\n",
      "round 0003 of 0003, seed=25\n",
      "splitted: (1283, 247), (321, 247)\n",
      "[0]\ttrain-error:0.138737\tvalid-error:0.261682\n",
      "Multiple eval metrics have been passed: 'valid-error' will be used for early stopping.\n",
      "\n",
      "Will train until valid-error hasn't improved in 100 rounds.\n",
      "[50]\ttrain-error:0.077942\tvalid-error:0.143302\n",
      "[100]\ttrain-error:0.052221\tvalid-error:0.124611\n",
      "[150]\ttrain-error:0.029618\tvalid-error:0.124611\n",
      "[200]\ttrain-error:0.016368\tvalid-error:0.11838\n",
      "[250]\ttrain-error:0.004677\tvalid-error:0.11215\n",
      "[300]\ttrain-error:0.003118\tvalid-error:0.105919\n",
      "[350]\ttrain-error:0.001559\tvalid-error:0.105919\n",
      "Stopping. Best iteration:\n",
      "[271]\ttrain-error:0.003118\tvalid-error:0.102804\n",
      "\n",
      "\n",
      "LightGBM: gbdt\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's binary_error: 0.140187\n",
      "[100]\tvalid_0's binary_error: 0.11838\n",
      "[150]\tvalid_0's binary_error: 0.105919\n",
      "[200]\tvalid_0's binary_error: 0.0996885\n",
      "[250]\tvalid_0's binary_error: 0.0903427\n",
      "[300]\tvalid_0's binary_error: 0.102804\n",
      "Early stopping, best iteration is:\n",
      "[218]\tvalid_0's binary_error: 0.0903427\n",
      "\n",
      "LightGBM: dart\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[50]\tvalid_0's binary_error: 0.127726\n",
      "[100]\tvalid_0's binary_error: 0.124611\n",
      "[150]\tvalid_0's binary_error: 0.109034\n",
      "[200]\tvalid_0's binary_error: 0.109034\n",
      "[250]\tvalid_0's binary_error: 0.105919\n",
      "[300]\tvalid_0's binary_error: 0.105919\n",
      "[350]\tvalid_0's binary_error: 0.109034\n",
      "Early stopping, best iteration is:\n",
      "[268]\tvalid_0's binary_error: 0.0996885\n",
      "\n",
      "Blending...\n",
      "load: ../data/boost/subm_2017-11-30-13-56_lgb_dart_02.csv, w=1.0\n",
      "load: ../data/boost/subm_2017-11-30-13-56_lgb_gbdt_03.csv, w=1.0\n",
      "load: ../data/boost/subm_2017-11-30-13-56_xgb_01.csv, w=1.0\n",
      "load: ../data/boost/subm_2017-11-30-13-56_xgb_03.csv, w=1.0\n",
      "load: ../data/boost/subm_2017-11-30-13-56_lgb_gbdt_01.csv, w=1.0\n",
      "load: ../data/boost/subm_2017-11-30-13-56_xgb_02.csv, w=1.0\n",
      "load: ../data/boost/subm_2017-11-30-13-56_lgb_dart_01.csv, w=1.0\n",
      "load: ../data/boost/subm_2017-11-30-13-56_lgb_gbdt_02.csv, w=1.0\n",
      "load: ../data/boost/subm_2017-11-30-13-56_lgb_dart_03.csv, w=1.0\n",
      "\n",
      "                                 bm_2017-11-30-13-56_lgb_dart_02  \\\n",
      "bm_2017-11-30-13-56_lgb_dart_02                         1.000000   \n",
      "bm_2017-11-30-13-56_lgb_gbdt_03                         0.970686   \n",
      "bm_2017-11-30-13-56_xgb_01                              0.957158   \n",
      "bm_2017-11-30-13-56_xgb_03                              0.934169   \n",
      "bm_2017-11-30-13-56_lgb_gbdt_01                         0.971595   \n",
      "bm_2017-11-30-13-56_xgb_02                              0.955403   \n",
      "bm_2017-11-30-13-56_lgb_dart_01                         0.978083   \n",
      "bm_2017-11-30-13-56_lgb_gbdt_02                         0.980920   \n",
      "bm_2017-11-30-13-56_lgb_dart_03                         0.986720   \n",
      "\n",
      "                                 bm_2017-11-30-13-56_lgb_gbdt_03  \\\n",
      "bm_2017-11-30-13-56_lgb_dart_02                         0.970686   \n",
      "bm_2017-11-30-13-56_lgb_gbdt_03                         1.000000   \n",
      "bm_2017-11-30-13-56_xgb_01                              0.966092   \n",
      "bm_2017-11-30-13-56_xgb_03                              0.973953   \n",
      "bm_2017-11-30-13-56_lgb_gbdt_01                         0.968121   \n",
      "bm_2017-11-30-13-56_xgb_02                              0.976525   \n",
      "bm_2017-11-30-13-56_lgb_dart_01                         0.952988   \n",
      "bm_2017-11-30-13-56_lgb_gbdt_02                         0.983819   \n",
      "bm_2017-11-30-13-56_lgb_dart_03                         0.982555   \n",
      "\n",
      "                                 bm_2017-11-30-13-56_xgb_01  \\\n",
      "bm_2017-11-30-13-56_lgb_dart_02                    0.957158   \n",
      "bm_2017-11-30-13-56_lgb_gbdt_03                    0.966092   \n",
      "bm_2017-11-30-13-56_xgb_01                         1.000000   \n",
      "bm_2017-11-30-13-56_xgb_03                         0.971375   \n",
      "bm_2017-11-30-13-56_lgb_gbdt_01                    0.965010   \n",
      "bm_2017-11-30-13-56_xgb_02                         0.969151   \n",
      "bm_2017-11-30-13-56_lgb_dart_01                    0.956005   \n",
      "bm_2017-11-30-13-56_lgb_gbdt_02                    0.963459   \n",
      "bm_2017-11-30-13-56_lgb_dart_03                    0.964347   \n",
      "\n",
      "                                 bm_2017-11-30-13-56_xgb_03  \\\n",
      "bm_2017-11-30-13-56_lgb_dart_02                    0.934169   \n",
      "bm_2017-11-30-13-56_lgb_gbdt_03                    0.973953   \n",
      "bm_2017-11-30-13-56_xgb_01                         0.971375   \n",
      "bm_2017-11-30-13-56_xgb_03                         1.000000   \n",
      "bm_2017-11-30-13-56_lgb_gbdt_01                    0.930964   \n",
      "bm_2017-11-30-13-56_xgb_02                         0.983266   \n",
      "bm_2017-11-30-13-56_lgb_dart_01                    0.911724   \n",
      "bm_2017-11-30-13-56_lgb_gbdt_02                    0.959958   \n",
      "bm_2017-11-30-13-56_lgb_dart_03                    0.951156   \n",
      "\n",
      "                                 bm_2017-11-30-13-56_lgb_gbdt_01  \\\n",
      "bm_2017-11-30-13-56_lgb_dart_02                         0.971595   \n",
      "bm_2017-11-30-13-56_lgb_gbdt_03                         0.968121   \n",
      "bm_2017-11-30-13-56_xgb_01                              0.965010   \n",
      "bm_2017-11-30-13-56_xgb_03                              0.930964   \n",
      "bm_2017-11-30-13-56_lgb_gbdt_01                         1.000000   \n",
      "bm_2017-11-30-13-56_xgb_02                              0.944688   \n",
      "bm_2017-11-30-13-56_lgb_dart_01                         0.985339   \n",
      "bm_2017-11-30-13-56_lgb_gbdt_02                         0.970874   \n",
      "bm_2017-11-30-13-56_lgb_dart_03                         0.969992   \n",
      "\n",
      "                                 bm_2017-11-30-13-56_xgb_02  \\\n",
      "bm_2017-11-30-13-56_lgb_dart_02                    0.955403   \n",
      "bm_2017-11-30-13-56_lgb_gbdt_03                    0.976525   \n",
      "bm_2017-11-30-13-56_xgb_01                         0.969151   \n",
      "bm_2017-11-30-13-56_xgb_03                         0.983266   \n",
      "bm_2017-11-30-13-56_lgb_gbdt_01                    0.944688   \n",
      "bm_2017-11-30-13-56_xgb_02                         1.000000   \n",
      "bm_2017-11-30-13-56_lgb_dart_01                    0.928277   \n",
      "bm_2017-11-30-13-56_lgb_gbdt_02                    0.979459   \n",
      "bm_2017-11-30-13-56_lgb_dart_03                    0.956853   \n",
      "\n",
      "                                 bm_2017-11-30-13-56_lgb_dart_01  \\\n",
      "bm_2017-11-30-13-56_lgb_dart_02                         0.978083   \n",
      "bm_2017-11-30-13-56_lgb_gbdt_03                         0.952988   \n",
      "bm_2017-11-30-13-56_xgb_01                              0.956005   \n",
      "bm_2017-11-30-13-56_xgb_03                              0.911724   \n",
      "bm_2017-11-30-13-56_lgb_gbdt_01                         0.985339   \n",
      "bm_2017-11-30-13-56_xgb_02                              0.928277   \n",
      "bm_2017-11-30-13-56_lgb_dart_01                         1.000000   \n",
      "bm_2017-11-30-13-56_lgb_gbdt_02                         0.959334   \n",
      "bm_2017-11-30-13-56_lgb_dart_03                         0.972311   \n",
      "\n",
      "                                 bm_2017-11-30-13-56_lgb_gbdt_02  \\\n",
      "bm_2017-11-30-13-56_lgb_dart_02                         0.980920   \n",
      "bm_2017-11-30-13-56_lgb_gbdt_03                         0.983819   \n",
      "bm_2017-11-30-13-56_xgb_01                              0.963459   \n",
      "bm_2017-11-30-13-56_xgb_03                              0.959958   \n",
      "bm_2017-11-30-13-56_lgb_gbdt_01                         0.970874   \n",
      "bm_2017-11-30-13-56_xgb_02                              0.979459   \n",
      "bm_2017-11-30-13-56_lgb_dart_01                         0.959334   \n",
      "bm_2017-11-30-13-56_lgb_gbdt_02                         1.000000   \n",
      "bm_2017-11-30-13-56_lgb_dart_03                         0.974850   \n",
      "\n",
      "                                 bm_2017-11-30-13-56_lgb_dart_03  \n",
      "bm_2017-11-30-13-56_lgb_dart_02                         0.986720  \n",
      "bm_2017-11-30-13-56_lgb_gbdt_03                         0.982555  \n",
      "bm_2017-11-30-13-56_xgb_01                              0.964347  \n",
      "bm_2017-11-30-13-56_xgb_03                              0.951156  \n",
      "bm_2017-11-30-13-56_lgb_gbdt_01                         0.969992  \n",
      "bm_2017-11-30-13-56_xgb_02                              0.956853  \n",
      "bm_2017-11-30-13-56_lgb_dart_01                         0.972311  \n",
      "bm_2017-11-30-13-56_lgb_gbdt_02                         0.974850  \n",
      "bm_2017-11-30-13-56_lgb_dart_03                         1.000000  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preview: \n",
      "         id  is_iceberg\n",
      "0  5941774d    0.196131\n",
      "1  4023181e    0.926411\n",
      "2  b20200e4    0.229445\n",
      "3  e7f018bb    0.957671\n",
      "4  4371c8c3    0.806921\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1504)\n",
    "target = 'is_iceberg'\n",
    "\n",
    "#Load data\n",
    "train, train_bands = read_jason(file='train.json', loc='../data/')\n",
    "test, test_bands = read_jason(file='test.json', loc='../data/')\n",
    "\n",
    "#train[\"band_1\"] = band[\"band_1\"][]\n",
    "\n",
    "train_X = process(df=train, bands=train_bands)\n",
    "train_y = train[target].values\n",
    "\n",
    "test_X = process(df=test, bands=test_bands)\n",
    "\n",
    "#results\n",
    "freq = pd.DataFrame()\n",
    "subms = []\n",
    "\n",
    "#training\n",
    "test_ratio = 0.2\n",
    "nr_runs = 3\n",
    "split_seed = 25\n",
    "kf = StratifiedShuffleSplit(n_splits=nr_runs, test_size=test_ratio, train_size=None, random_state=split_seed)\n",
    "\n",
    "for r, (train_index, test_index) in enumerate(kf.split(train_X, train_y)):\n",
    "    print('\\nround {:04d} of {:04d}, seed={}'.format(r+1, nr_runs, split_seed))\n",
    "\n",
    "    tmp = dt.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "\n",
    "    x1, x2 = train_X[train_index], train_X[test_index]\n",
    "    y1, y2 = train_y[train_index], train_y[test_index]\n",
    "    #x1, x2, y1, y2 = train_test_split(train_X, train_y, test_size=test_ratio, random_state=split_seed + r)\n",
    "    print('splitted: {0}, {1}'.format(x1.shape, x2.shape), flush=True)\n",
    "    test_X_dup = test_X.copy()\n",
    "\n",
    "    #XGB\n",
    "    xgb_train = xgb.DMatrix(x1, y1)\n",
    "    xgb_valid = xgb.DMatrix(x2, y2)\n",
    "    #\n",
    "    watchlist = [(xgb_train, 'train'), (xgb_valid, 'valid')]\n",
    "    params = {'eta': 0.02, 'max_depth': 4, 'subsample': 0.9, 'colsample_bytree': 0.9, 'objective': 'binary:logistic', 'seed': 99, 'silent': True}\n",
    "    params['eta'] = 0.03\n",
    "    params['max_depth'] = 4\n",
    "    params['subsample'] = 0.9\n",
    "    params['eval_metric'] = 'error'\n",
    "    params['colsample_bytree'] = 0.8\n",
    "    params['colsample_bylevel'] = 0.8\n",
    "    params['max_delta_step'] = 3\n",
    "    #params['gamma'] = 5.0\n",
    "    #params['labmda'] = 1\n",
    "    params['scale_pos_weight'] = 1.0\n",
    "    params['seed'] = split_seed + r\n",
    "    nr_round = 2000\n",
    "    min_round = 100\n",
    "\n",
    "    model1 = xgb.train(params, \n",
    "                       xgb_train, \n",
    "                       nr_round,  \n",
    "                       watchlist, \n",
    "                       verbose_eval=50, \n",
    "                       early_stopping_rounds=min_round)\n",
    "\n",
    "    pred_xgb = model1.predict(xgb.DMatrix(test_X_dup), ntree_limit=model1.best_ntree_limit+45)\n",
    "\n",
    "    #\n",
    "    file = STORE_PATH + 'subm_{}_xgb_{:02d}.csv'.format(tmp, r+1)\n",
    "    subm = pd.DataFrame({'id': test['id'].values, target: pred_xgb})\n",
    "    subm.to_csv(file, index=False, float_format='%.6f')\n",
    "    subms.append(file)    \n",
    "\n",
    "    ##LightGBM\n",
    "    lgb_train = lgb.Dataset(x1, label=y1, free_raw_data=False)\n",
    "    lgb_valid = lgb.Dataset(x2, label=y2, reference=lgb_train, free_raw_data=False)\n",
    "    #gbdt\n",
    "    params = {'learning_rate': 0.02, 'max_depth': 4, 'boosting': 'gbdt', 'objective': 'binary', 'is_training_metric': False, 'seed': 99}\n",
    "    params['boosting'] = 'gbdt'\n",
    "    params['metric'] = 'binary_error'\n",
    "    params['learning_rate'] = 0.03\n",
    "    params['max_depth'] = 5\n",
    "    params['num_leaves'] = 16 # higher number of leaves\n",
    "    params['feature_fraction'] = 0.8 # Controls overfit\n",
    "    params['bagging_fraction'] = 0.9    \n",
    "    params['bagging_freq'] = 3\n",
    "    params['seed'] = split_seed + r\n",
    "    #\n",
    "    params['verbose'] = -1\n",
    "\n",
    "    file = STORE_PATH + 'subm_{}_lgb_{}_{:02d}.csv'.format(tmp, params['boosting'], r+1)\n",
    "    subms.append(file)\n",
    "\n",
    "    pred, f_tmp = run_lgb(params=params, \n",
    "                          lgb_train=lgb_train, \n",
    "                          lgb_valid=lgb_valid, \n",
    "                          lgb_test=test_X_dup, \n",
    "                          test_ids=test['id'].values, \n",
    "                          nr_round=nr_round,\n",
    "                          min_round=min_round, \n",
    "                          file=file)\n",
    "\n",
    "    ##LightGBM\n",
    "    #dart\n",
    "    params = {'learning_rate': 0.02, 'max_depth': 4, 'boosting': 'gbdt', 'objective': 'binary', 'is_training_metric': False, 'seed': 99}\n",
    "    params['boosting'] = 'dart'\n",
    "    params['metric'] = 'binary_error'\n",
    "    params['learning_rate'] = 0.04\n",
    "    params['max_depth'] = 5\n",
    "    params['num_leaves'] = 16 # higher number of leaves\n",
    "    params['feature_fraction'] = 0.8 # Controls overfit\n",
    "    params['bagging_fraction'] = 0.9    \n",
    "    params['bagging_freq'] = 3\n",
    "    params['seed'] = split_seed + r\n",
    "    #dart\n",
    "    params['drop_rate'] = 0.1\n",
    "    params['skip_drop'] = 0.5\n",
    "    params['max_drop'] = 10\n",
    "    params['verbose'] = -1 \n",
    "\n",
    "    file = STORE_PATH + 'subm_{}_lgb_{}_{:02d}.csv'.format(tmp, params['boosting'], r+1)\n",
    "    subms.append(file)\n",
    "\n",
    "    pred, f_tmp = run_lgb(params=params, \n",
    "                          lgb_train=lgb_train, \n",
    "                          lgb_valid=lgb_valid, \n",
    "                          lgb_test=test_X_dup, \n",
    "                          test_ids=test['id'].values, \n",
    "                          nr_round=nr_round, \n",
    "                          min_round=min_round, \n",
    "                          file=file)\n",
    "\n",
    "\n",
    "#blending\n",
    "preds = {k: 1.0 for k in subms}\n",
    "save_blend(preds=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
